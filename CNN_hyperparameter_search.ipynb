{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will run replicates of a model and perform a random search of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_handling\n",
    "import training_and_validation\n",
    "import hyperparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import importlib\n",
    "import glob\n",
    "\n",
    "from models import PD_CNN, ResNet, ResidualBlock, PD_LSTM\n",
    "import torch\n",
    "import training_and_validation\n",
    "from training_and_validation import loso_cross_validation, train, validate, cross_train, train_with_validation\n",
    "from result_visualization import run_chi_squared_test, plot_confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import data_handling\n",
    "import models\n",
    "import result_visualization\n",
    "import random\n",
    "import hyperparam_tuning\n",
    "import os\n",
    "from sys import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find training data, make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the device to cuda:0 \n",
    "if torch.cuda.is_available():  \n",
    "  device = \"cuda:0\" \n",
    "else:  \n",
    "  device = \"cpu\" \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#please specify where you keep your training dataset\n",
    "training_data_src =  './Data/UNM/whole_dataset/'\n",
    "\n",
    "############ create list of subject numbers to leave out ###############################\n",
    "files = glob.glob(training_data_src + '*.csv')\n",
    "leave_one_out_list = []\n",
    "for file in files:  \n",
    "  if platform=='win32':\n",
    "    leave_one_out_list.append(file.split('\\\\')[-1])#.split('_')[1]) #remove hashtags to return to UNM dataset\n",
    "  elif platform=='linux' or platform=='linux2' or platform.startswith('darwin'):\n",
    "    leave_one_out_list.append(file.split('/')[-1])\n",
    "  else:\n",
    "    assert ValueError, 'operating system not identified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making a custom Dataset\n",
      "there are this many items in the list of data  118\n",
      "there are this many items in the list of labels  118\n",
      "The length of the lists of channels means and stds is  60\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#make training dataset\n",
    "if device == 'cuda:0':\n",
    "    print('making a TensorDataset')\n",
    "    training_dataset = data_handling.make_data_into_tensor(data_path=training_data_src, device=device, )\n",
    "else:\n",
    "    print('making a custom Dataset')\n",
    "    training_dataset = data_handling.EEGDataset(data_path=training_data_src, chunk_size=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose model and search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model = 'Transformer'\n",
    "experiment_name = experiment_model+'_adjustable_pilot_hyperparameter_search'\n",
    "training_target_dir = './training_results/'+experiment_model+'/'+experiment_name+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------running replicate # 0 -------------------------\n",
      "hyperparameter configuration:  {'batch_size': 21, 'epochs': 2, 'learning_rate': 0.00029630094561443733, 'num_heads': 6, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 4\n",
      "val_dataloader 2\n",
      "in transformer, d_model: 60 n_head: 6 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 1 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:46<02:19, 46.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 5\n",
      "val_dataloader 2\n",
      "in transformer, d_model: 60 n_head: 6 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 1 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [01:31<01:31, 45.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 5\n",
      "val_dataloader 2\n",
      "in transformer, d_model: 60 n_head: 6 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 1 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [02:05<00:40, 40.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 5\n",
      "val_dataloader 1\n",
      "in transformer, d_model: 60 n_head: 6 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 1 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:40<00:00, 40.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct subject classifications:  0\n",
      "total incorrect subject classifications:  4\n",
      "total unsure subject classifications:  0\n",
      "total true postives (epochs) 0\n",
      "total false postives (epochs) 41\n",
      "total true negatives (epochs) 5\n",
      "total false negatives (epochs) 72\n",
      "----------------------------------------------------------------\n",
      "accuracy 0.0423728813559322  f1  0 sensitivity 0.0 specificity 0.10869565217391304\n",
      "./training_results/Transformer/Transformer_adjustable_pilot_hyperparameter_search/\n",
      "-----------------running replicate # 1 -------------------------\n",
      "hyperparameter configuration:  {'batch_size': 28, 'epochs': 2, 'learning_rate': 0.001309700938030553, 'num_heads': 4, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 3\n",
      "val_dataloader 2\n",
      "in transformer, d_model: 60 n_head: 4 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 1 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:16<00:49, 16.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 4\n",
      "val_dataloader 2\n",
      "in transformer, d_model: 60 n_head: 4 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 1 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:34<00:34, 17.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 4\n",
      "val_dataloader 2\n",
      "in transformer, d_model: 60 n_head: 4 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 1 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:51<00:17, 17.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 4\n",
      "val_dataloader 1\n",
      "in transformer, d_model: 60 n_head: 4 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 1 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:12<00:00, 18.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct subject classifications:  0\n",
      "total incorrect subject classifications:  4\n",
      "total unsure subject classifications:  0\n",
      "total true postives (epochs) 1\n",
      "total false postives (epochs) 40\n",
      "total true negatives (epochs) 6\n",
      "total false negatives (epochs) 71\n",
      "----------------------------------------------------------------\n",
      "accuracy 0.059322033898305086  f1  0.017699115044247787 sensitivity 0.013888888888888888 specificity 0.13043478260869565\n",
      "./training_results/Transformer/Transformer_adjustable_pilot_hyperparameter_search/\n",
      "-----------------running replicate # 2 -------------------------\n",
      "hyperparameter configuration:  {'batch_size': 30, 'epochs': 1, 'learning_rate': 0.0002367004488439419, 'num_heads': 2, 'num_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 3\n",
      "val_dataloader 2\n",
      "in transformer, d_model: 60 n_head: 2 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 6 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [01:00<03:00, 60.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 3\n",
      "val_dataloader 2\n",
      "in transformer, d_model: 60 n_head: 2 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 6 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [02:08<02:10, 65.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 3\n",
      "val_dataloader 2\n",
      "in transformer, d_model: 60 n_head: 2 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 6 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [03:15<01:06, 66.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting torch dataset\n",
      "train_dataloader 4\n",
      "val_dataloader 1\n",
      "in transformer, d_model: 60 n_head: 2 max_len: 5000 seq_len: 2500 ffn_hidden: 128 n_layers: 6 drop_prob: 0.1 details: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [03:38<01:12, 72.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhyperparam_tuning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#run cross validation for N random configurations of hyperparameters\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m hyperparam_tuning\u001b[39m.\u001b[39;49mperform_random_hyperparameter_search(training_dataset, leave_one_out_list, sample_size\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m                                                        learning_rate_min_max\u001b[39m=\u001b[39;49m(\u001b[39m0.00001\u001b[39;49m,\u001b[39m0.01\u001b[39;49m), batch_min_max\u001b[39m=\u001b[39;49m(\u001b[39m2\u001b[39;49m,\u001b[39m32\u001b[39;49m), \n\u001b[1;32m     10\u001b[0m                                                        epoch_min_max\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m,\u001b[39m3\u001b[39;49m), search_title\u001b[39m=\u001b[39;49mexperiment_name, save_path\u001b[39m=\u001b[39;49mtraining_target_dir,\n\u001b[1;32m     11\u001b[0m                                                         model_type\u001b[39m=\u001b[39;49mexperiment_model, device\u001b[39m=\u001b[39;49mdevice, rand_seed\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m) \u001b[39m#13 worked for one epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/EEG_analysis/DL_for_EEG/hyperparam_tuning.py:102\u001b[0m, in \u001b[0;36mperform_random_hyperparameter_search\u001b[0;34m(EEG_dataset, leave_one_out_list, sample_size, search_title, save_path, batch_min_max, epoch_min_max, learning_rate_min_max, model_type, supress_output, device, rand_seed)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mhyperparameter configuration: \u001b[39m\u001b[39m'\u001b[39m, configuration_dict)\n\u001b[1;32m    101\u001b[0m \u001b[39m#perform cross validation\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m cv_log, total_metrics \u001b[39m=\u001b[39m loso_cross_validation(filename_list\u001b[39m=\u001b[39;49mleave_one_out_list, EEG_whole_Dataset\u001b[39m=\u001b[39;49mEEG_dataset, model_type\u001b[39m=\u001b[39;49mmodel_type, \n\u001b[1;32m    103\u001b[0m                                             epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    104\u001b[0m                                             device\u001b[39m=\u001b[39;49mdevice, supress_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, configuration\u001b[39m=\u001b[39;49mconfiguration_dict)\n\u001b[1;32m    106\u001b[0m \u001b[39m#save results\u001b[39;00m\n\u001b[1;32m    107\u001b[0m make_csv_from_log(cv_log, total_metrics, filename\u001b[39m=\u001b[39mconfiguration_string, save_path\u001b[39m=\u001b[39msave_path)\n",
      "File \u001b[0;32m~/Desktop/EEG_analysis/DL_for_EEG/training_and_validation.py:427\u001b[0m, in \u001b[0;36mloso_cross_validation\u001b[0;34m(filename_list, EEG_whole_Dataset, configuration, model_type, epochs, batch_size, num_workers, learning_rate, chunk_size, device, supress_output)\u001b[0m\n\u001b[1;32m    424\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    426\u001b[0m \u001b[39m#perform one fold of training and validation\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m TP, FP, TN, FN, vote, model \u001b[39m=\u001b[39m cross_train(model, train_dataloader, val_dataloader, epochs\u001b[39m=\u001b[39;49mepochs, learning_rate\u001b[39m=\u001b[39;49mlearning_rate, threshold\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, chunk_size\u001b[39m=\u001b[39;49mchunk_size, device\u001b[39m=\u001b[39;49mdevice, supress_output\u001b[39m=\u001b[39;49msupress_output)\n\u001b[1;32m    429\u001b[0m \u001b[39m#add the metrics to the lists\u001b[39;00m\n\u001b[1;32m    430\u001b[0m true_positives \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m TP\n",
      "File \u001b[0;32m~/Desktop/EEG_analysis/DL_for_EEG/training_and_validation.py:241\u001b[0m, in \u001b[0;36mcross_train\u001b[0;34m(model, train_dataloader, val_dataloader, epochs, learning_rate, num_workers, threshold, chunk_size, device, supress_output)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m#Regularization Replaces pow(2.0) with abs() for L1 regularization\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m#l2_lambda = 0.001\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39m#l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m#print('labels', labels)\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m#loss + backward + optimize\u001b[39;00m\n\u001b[1;32m    240\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs,labels) \u001b[39m#+ l2_lambda*l2_norm\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    242\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    244\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/EEG_analysis/DL_for_EEG/DL_EEG_venv/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/EEG_analysis/DL_for_EEG/DL_EEG_venv/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(hyperparam_tuning)\n",
    "importlib.reload(training_and_validation)\n",
    "importlib.reload(models)\n",
    "import models\n",
    "import training_and_validation\n",
    "import hyperparam_tuning\n",
    "#run cross validation for N random configurations of hyperparameters\n",
    "hyperparam_tuning.perform_random_hyperparameter_search(training_dataset, leave_one_out_list, sample_size=60,\n",
    "                                                       learning_rate_min_max=(0.00001,0.01), batch_min_max=(2,32), \n",
    "                                                       epoch_min_max=(1,3), search_title=experiment_name, save_path=training_target_dir,\n",
    "                                                        model_type=experiment_model, device=device, rand_seed=5) #13 worked for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_EEG_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

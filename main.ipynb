{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ricksugden/Desktop/EEG_analysis/DL_for_EEG\n",
      "Already up to date.\n",
      "Result: CD'ed into our DL_for_EEG repo and did a git pull\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sys import platform\n",
    "\n",
    "if platform == 'win32':\n",
    "    if os.path.isdir(r'C:\\Users\\Researcher\\Desktop\\EEG_analysis\\DL_for_EEG'):\n",
    "        %cd r'C:\\Users\\Researcher\\Desktop\\EEG_analysis\\DL_for_EEG'\n",
    "        !git pull \n",
    "        print(\"Result: CD'ed into our DL_for_EEG repo and did a git pull\")\n",
    "\n",
    "    elif os.path.isdir(r'C:\\Users\\Researcher\\Desktop\\EEG_analysis\\DL_for_EEG'):\n",
    "        %cd r'C:\\Users\\Researcher\\Desktop\\EEG_analysis'\n",
    "        github_token = \"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\"\n",
    "        !git clone https://github.com/RickSugden/DL_for_EEG.git \n",
    "        %cd /Users/ricksugden/Desktop/EEG_analysis/DL_for_EEG\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "elif platform == 'linux' or platform=='linux2' or platform.startswith('darwin'):\n",
    "    \n",
    "    if os.path.isdir(\"/Users/ricksugden/Desktop/EEG_analysis/DL_for_EEG\"):\n",
    "        %cd \"/Users/ricksugden/Desktop/EEG_analysis/DL_for_EEG\"\n",
    "        !git pull \n",
    "        print(\"Result: CD'ed into our DL_for_EEG repo and did a git pull\")\n",
    "    elif os.path.isdir(\"/content/\"):\n",
    "        github_token = \"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\"\n",
    "        !git clone https://$github_token@github.com/RickSugden/DL_for_EEG.git\n",
    "        %cd /Users/ricksugden/Desktop/EEG_analysis/DL_for_EEG\n",
    "    else:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import importlib\n",
    "import glob\n",
    "\n",
    "from models import PD_CNN, ResNet, ResidualBlock, PD_LSTM\n",
    "import torch\n",
    "import training_and_validation\n",
    "from training_and_validation import loso_cross_validation, train, validate, cross_train, train_with_validation\n",
    "from result_visualization import run_chi_squared_test, plot_confusion_matrix\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import data_handling\n",
    "import models\n",
    "import result_visualization\n",
    "import random\n",
    "import hyperparam_tuning\n",
    "import os\n",
    "from sys import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'hyperparam_tuning' from 'c:\\\\Users\\\\Diamandis Lab II\\\\Desktop\\\\EEG_analysis\\\\DL_for_EEG\\\\hyperparam_tuning.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(training_and_validation)\n",
    "importlib.reload(data_handling)\n",
    "#importlib.reload(models) #turns out reloading classes in jupyter isn't so easy\n",
    "importlib.reload(result_visualization)\n",
    "importlib.reload(hyperparam_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the device to cuda:0 \n",
    "if torch.cuda.is_available():  \n",
    "  device = \"cuda:0\" \n",
    "else:  \n",
    "  device = \"cpu\" \n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV UNM No Leak Experiment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are this many items in the list of data  118\n",
      "there are this many items in the list of labels  118\n",
      "The length of the lists of channels means and stds is  60\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_workers = 2\n",
    "chunk_size = 2500\n",
    "#locate the raw data\n",
    "data_src =  './Data/UNM/small_subset/'\n",
    "\n",
    "############ create list of subject numbers to leave out ###############################\n",
    "files = glob.glob(data_src + '*.csv')\n",
    "leave_one_out_list = []\n",
    "for file in files:  \n",
    "  leave_one_out_list.append(file.split('/')[-1])#.split('_')[1]) #remove hashtags to return to UNM dataset\n",
    "\n",
    "############# create dataset of all data ############################\n",
    "EEG_whole_Dataset = data_handling.EEGDataset(data_path=data_src, chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are this many batches in the training dataloader: 14\n",
      "there are this many batches in the validation dataloader:  2\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADER\n",
    "################ CREATE DATALOADER  ############################################\n",
    "#define the train test split\n",
    "train_size = int(0.90 * len(EEG_whole_Dataset))\n",
    "val_size = len(EEG_whole_Dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(EEG_whole_Dataset, [train_size, val_size],generator=torch.Generator().manual_seed(402))\n",
    "\n",
    "#create a respective dataloader out of the test/train split\n",
    "training_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True, num_workers=num_workers)\n",
    "\n",
    "#print(next(iter(training_dataloader))[0].size())\n",
    "print('there are this many batches in the training dataloader:',len(training_dataloader))\n",
    "#print(next(iter(validation_dataloader))[0].size())\n",
    "print('there are this many batches in the validation dataloader: ',len(validation_dataloader))\n",
    "#the trainlaoder has 91 batches, the valloader has 16 batches\n",
    "#1.5 Minute runtime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PD_CNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "importlib.reload(models)\n",
    "import models\n",
    "from models import PD_CNN, ResNet, ResidualBlock, PD_LSTM, VGG13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 60, 2500])\n",
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# confirm model is buildable\n",
    "import torch\n",
    "import models\n",
    "from models import PD_CNN, ResNet, ResidualBlock, PD_LSTM\n",
    "input_tensor = torch.rand([8,60,2500]).to(device) #of the form [batch_size, channels, epoch_length]\n",
    "print(input_tensor.size())\n",
    "network = models.VGG13(num_channels=60, num_filters=1).to(device)\n",
    "output_tensor = network(input_tensor)\n",
    "print((output_tensor.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Train and Validate methods work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batch:    86] average training loss: 0.603\n",
      "true positives:  0\n",
      "false positives:  0\n",
      "true negatives:  57\n",
      "false negatives 19\n",
      "The vote was:  Correct\n",
      "Finished Training Session\n",
      "The training loss at the end of this session is:  0.4973051846027374\n"
     ]
    }
   ],
   "source": [
    "# Confirm training loop runs\n",
    "train_model, training_loss_tracker, val_loss_tracker = train_with_validation(network, train_dataloader=training_dataloader, val_dataloader=validation_dataloader, epochs=1, device=device)\n",
    "# validate(model=train_model, valloader=valloader, device=device)\n",
    "#2 Minute runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vote was:  Correct\n",
      "True Positives:  0\n",
      "False Positives:  0\n",
      "True Negatives:  57\n",
      "False Negatives:  19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 0,\n",
       " 57,\n",
       " 19,\n",
       " 'Correct',\n",
       " PD_CNN(\n",
       "   (conv1): Conv1d(60, 21, kernel_size=(20,), stride=(1,))\n",
       "   (norm1): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (maxpool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "   (conv2): Conv1d(21, 42, kernel_size=(10,), stride=(1,))\n",
       "   (norm2): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (maxpool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "   (conv3): Conv1d(42, 42, kernel_size=(10,), stride=(1,))\n",
       "   (norm3): BatchNorm1d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (maxpool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "   (conv4): Conv1d(42, 64, kernel_size=(5,), stride=(1,))\n",
       "   (norm4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (maxpool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "   (relu): LeakyReLU(negative_slope=0.1)\n",
       "   (fc1): Linear(in_features=448, out_features=256, bias=True)\n",
       "   (dropout1): Dropout(p=0.5, inplace=False)\n",
       "   (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "   (dropout2): Dropout(p=0.5, inplace=False)\n",
       "   (fc3): Linear(in_features=64, out_features=16, bias=True)\n",
       "   (fc4): Linear(in_features=16, out_features=2, bias=True)\n",
       "   (softmax): Softmax(dim=1)\n",
       " ))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm cross validation \"Cross_train wor\n",
    "#2 minute runtimeks\"\n",
    "cross_train(train_dataloader=training_dataloader, val_dataloader=validation_dataloader, epochs=1, device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validate\n",
    "cv_log, total_metrics = loso_cross_validation(filename_list=leave_one_out_list, EEG_whole_Dataset=EEG_whole_Dataset, model='CNN',epochs=1, device=device, supress_output=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data_handling)\n",
    "import data_handling\n",
    "importlib.reload(training_and_validation)\n",
    "importlib.reload(hyperparam_tuning)\n",
    "import training_and_validation\n",
    "import hyperparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#please specify where you keep your training dataset\n",
    "training_data_src =  './Data/UNM/whole_dataset/'\n",
    "\n",
    "############ create list of subject numbers to leave out ###############################\n",
    "files = glob.glob(training_data_src + '*.csv')\n",
    "leave_one_out_list = []\n",
    "for file in files:  \n",
    "  if platform=='win32':\n",
    "    leave_one_out_list.append(file.split('\\\\')[-1])#.split('_')[1]) #remove hashtags to return to UNM dataset\n",
    "  elif platform=='linux' or platform=='linux2' or platform.startswith('darwin'):\n",
    "    leave_one_out_list.append(file.split('/')[-1])\n",
    "  else:\n",
    "    assert ValueError, 'operating system not identified'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making a small TensorDataset\n",
      "there are this many items in the list of data  162\n",
      "there are this many items in the list of labels  162\n",
      "The length of the lists of channels means and stds is  60\n",
      "The final shape of the tensor dataset is  torch.Size([162, 2500, 60])\n",
      "The final shape of the tensor labels is  torch.Size([162])\n",
      "moving tensor dataset to gpu\n"
     ]
    }
   ],
   "source": [
    "#make training dataset\n",
    "if device == 'cuda:0':\n",
    "    print('making a small TensorDataset')\n",
    "    small_training_dataset = data_handling.make_data_into_tensor(data_path='./Data/UNM/small_subset/', device=device)\n",
    "else:\n",
    "    print('making a custom Dataset')\n",
    "    small_training_dataset = data_handling.EEGDataset(data_path=training_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making a TensorDataset\n",
      "there are this many items in the list of data  2122\n",
      "there are this many items in the list of labels  2122\n",
      "The length of the lists of channels means and stds is  60\n",
      "The final shape of the tensor dataset is  torch.Size([2122, 2500, 60])\n",
      "The final shape of the tensor labels is  torch.Size([2122])\n",
      "moving tensor dataset to gpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#make training dataset\n",
    "if device == 'cuda:0':\n",
    "    print('making a TensorDataset')\n",
    "    training_dataset = data_handling.make_data_into_tensor(data_path=training_data_src, device=device)\n",
    "else:\n",
    "    print('making a custom Dataset')\n",
    "    training_dataset = data_handling.EEGDataset(data_path=training_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_model = 'EEGNet'\n",
    "experiment_name = experiment_model+'_hyperparameter_search'\n",
    "training_target_dir = './training_results/'+experiment_model+'/'+experiment_name+'/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------running replicate # 0 -------------------------\n",
      "hyperparameter configuration:  EEGNet_batch_size_59_epochs_23_learning_rate_5e-05\n",
      "tracker for the subjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/52 [00:57<49:08, 57.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/52 [01:55<48:06, 57.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/52 [02:53<47:09, 57.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/52 [03:50<46:09, 57.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 5/52 [04:47<45:01, 57.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/52 [05:44<43:41, 57.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7/52 [06:39<42:28, 56.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 8/52 [07:36<41:36, 56.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 9/52 [08:32<40:20, 56.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 10/52 [09:24<38:30, 55.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 11/52 [10:16<37:01, 54.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 12/52 [11:11<36:11, 54.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 13/52 [12:08<35:49, 55.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 14/52 [13:04<35:12, 55.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting tensor dataset\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(hyperparam_tuning)\n",
    "importlib.reload(training_and_validation)\n",
    "import training_and_validation\n",
    "import hyperparam_tuning\n",
    "#run cross validation for N random configurations of hyperparameters\n",
    "hyperparam_tuning.perform_random_hyperparameter_search(training_dataset, leave_one_out_list, sample_size=60,\n",
    "                                                       learning_rate_min_max=(0.00001,0.01), batch_min_max=(2,64), \n",
    "                                                       epoch_min_max=(5,50), search_title=experiment_name, save_path=training_target_dir,\n",
    "                                                        model_type=experiment_model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best file was:  CNN_batch_size_9_epochs_1_learning_rate_0.03232.csv\n"
     ]
    }
   ],
   "source": [
    "#determine the best hyperparameter combo\n",
    "\n",
    "best_file = hyperparam_tuning.find_best_hyperparmeter_combo(dir=training_target_dir)\n",
    "print('the best file was: ', best_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the best hyperparameter combo, train replicates and test on the external dataset\n",
    "replicates = 2\n",
    "best_batch_size = int(best_file.split('_')[3])\n",
    "best_epochs = int(best_file.split('_')[5])\n",
    "best_learning_rate = float(best_file.split('_')[8].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNN_batch_size_9_epochs_1_learning_rate_0.03232.csv'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are this many items in the list of data  138\n",
      "there are this many items in the list of labels  138\n",
      "The length of the lists of channels means and stds is  60\n",
      "the length of the training dataloader is:  14\n",
      "the length of the testing dataloader is:  16\n"
     ]
    }
   ],
   "source": [
    "#make the training dataloader according to the best batch size\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=best_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#make the testing dataset\n",
    "testing_data_src =  './Data/UI/small_subset/'\n",
    "testing_dataset = data_handling.EEGDataset(data_path=testing_data_src)\n",
    "\n",
    "#make the testing dataloader\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=best_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "print('the length of the training dataloader is: ', len(training_dataloader))\n",
    "print('the length of the testing dataloader is: ', len(testing_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PD_1661.csv', 'CTL_1201.csv', 'CTL_1081.csv', 'PD_1681.csv']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_filename_list = []\n",
    "for file in glob.glob(testing_data_src + '*.csv'):\n",
    "    testing_filename_list.append(file.split('/')[-1])\n",
    "\n",
    "testing_filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Session\n",
      "The training loss at the end of this session is:  0.6753301620483398\n",
      "Performing testing on subject number:  PD_1661\n",
      "there are this many items in the list of data  25\n",
      "there are this many items in the list of labels  25\n",
      "The length of the lists of channels means and stds is  60\n",
      "Performing testing on subject number:  CTL_1201\n",
      "there are this many items in the list of data  36\n",
      "there are this many items in the list of labels  36\n",
      "The length of the lists of channels means and stds is  60\n",
      "Performing testing on subject number:  CTL_1081\n",
      "there are this many items in the list of data  52\n",
      "there are this many items in the list of labels  52\n",
      "The length of the lists of channels means and stds is  60\n",
      "Performing testing on subject number:  PD_1681\n",
      "there are this many items in the list of data  25\n",
      "there are this many items in the list of labels  25\n",
      "The length of the lists of channels means and stds is  60\n",
      "[47, 86, 2, 3]\n",
      "replicate  0  complete\n",
      "----------------------------------------------------------------\n",
      "Finished Training Session\n",
      "The training loss at the end of this session is:  0.6806683540344238\n",
      "Performing testing on subject number:  PD_1661\n",
      "there are this many items in the list of data  25\n",
      "there are this many items in the list of labels  25\n",
      "The length of the lists of channels means and stds is  60\n",
      "Performing testing on subject number:  CTL_1201\n",
      "there are this many items in the list of data  36\n",
      "there are this many items in the list of labels  36\n",
      "The length of the lists of channels means and stds is  60\n",
      "Performing testing on subject number:  CTL_1081\n",
      "there are this many items in the list of data  52\n",
      "there are this many items in the list of labels  52\n",
      "The length of the lists of channels means and stds is  60\n",
      "Performing testing on subject number:  PD_1681\n",
      "there are this many items in the list of data  25\n",
      "there are this many items in the list of labels  25\n",
      "The length of the lists of channels means and stds is  60\n",
      "[50, 88, 0, 0]\n",
      "replicate  1  complete\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(training_and_validation)\n",
    "import os\n",
    "\n",
    "#if folder does not exist for this experiment name, make one\n",
    "if not os.path.exists('./testing_results/'+experiment_model+'/'+experiment_name+'/'):\n",
    "    os.makedirs('./testing_results/'+experiment_model+'/'+experiment_name+'/')\n",
    "\n",
    "#set target directory\n",
    "target_dir = './testing_results/'+experiment_model+'/'+ experiment_name+'/'\n",
    "\n",
    "#make the training dataset\n",
    "for i in range(replicates):\n",
    "\n",
    "    #intialize a model\n",
    "    untrained_model = training_and_validation.initialize_model(model_type=experiment_model, device=device)\n",
    "\n",
    "    #train the model\n",
    "    trained_model, training_loss_tracker = training_and_validation.train(untrained_model, train_dataloader=training_dataloader, learning_rate=best_learning_rate, epochs=best_epochs, device=device)\n",
    "\n",
    "    #test the model\n",
    "    epoch_conf_mat, sub_conf_mat, votes = training_and_validation.test_subjectwise(trained_model=trained_model, test_data_src=testing_data_src, filename_list=testing_filename_list)\n",
    "\n",
    "    #save the results\n",
    "    csv = training_and_validation.save_testing_results(epoch_conf_mat, sub_conf_mat, votes, results_dir=target_dir , experiment_name=experiment_name, replicate=i)\n",
    "    csv.close()\n",
    "    print('replicate ', i, ' complete')\n",
    "    print('----------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_metrics_from_testing_folder(path):\n",
    "    \n",
    "    accuracy_list, f1_score_list, sensitivity_list, specificity_list, precision_list, recall_list = [], [], [], [], [], []\n",
    "    \n",
    "    #if the path does not end in a slash, add one\n",
    "    if path[-1] != '/':\n",
    "        path = path + '/'\n",
    "    \n",
    "    #get all files in the folder\n",
    "    path = glob.glob(path + '*.csv')\n",
    "    \n",
    "    for file in path:\n",
    "        \n",
    "        #open the file\n",
    "        csv = open(file, 'r')\n",
    "\n",
    "        #first line is labels\n",
    "        _ = csv.readline().split(',')\n",
    "\n",
    "        #second line is the data\n",
    "        data = csv.readline().split(',')\n",
    "\n",
    "        #if it ends with \\n, remove the \\n\n",
    "        if data[-1] == '\\n':\n",
    "            data = data[:-1]\n",
    "        \n",
    "        #convert to ints\n",
    "        TP, FP, TN, FN = data\n",
    "        TP, FP, TN, FN = int(TP), int(FP), int(TN), int(FN)\n",
    "\n",
    "        #compute the accuracy, sensitivity, specificity, precision, recall, and f1 score\n",
    "        accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
    "        sensitivity = TP/(TP+FN)\n",
    "        specificity = TN/(TN+FP)\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        f1_score = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "        #append to the lists\n",
    "        accuracy_list.append(accuracy)\n",
    "        f1_score_list.append(f1_score)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "    #return the average and std of each metric\n",
    "    return (np.mean(accuracy_list), np.std(accuracy_list)), (np.mean(f1_score_list), np.std(f1_score_list)), (np.mean(sensitivity_list), np.std(sensitivity_list)), (np.mean(specificity_list), np.std(specificity_list)), (np.mean(precision_list), np.std(precision_list)), (np.mean(recall_list), np.std(recall_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment name:  CNN_hyperparameter_search1\n",
      "accuracy:  0.36232 +/- 0.0\n",
      "f1 score:  0.53191 +/- 0.0\n",
      "sensitivity:  1.0 +/- 0.0\n",
      "specificity:  0.0 +/- 0.0\n",
      "precision:  0.36232 +/- 0.0\n",
      "recall:  1.0 +/- 0.0\n",
      "----------------------------------------------------------------\n",
      "experiment name:  LSTM_hyperparameter_search1\n",
      "accuracy:  0.47464 +/- 0.11232\n",
      "f1 score:  0.43585 +/- 0.09103\n",
      "sensitivity:  0.64 +/- 0.34\n",
      "specificity:  0.38068 +/- 0.36932\n",
      "precision:  0.38285 +/- 0.02256\n",
      "recall:  0.64 +/- 0.34\n",
      "----------------------------------------------------------------\n",
      "  model  accuracy  f1_score  sensitivity  specificity  precision  recall\n",
      "0   CNN  0.362319  0.531915         1.00     0.000000   0.362319    1.00\n",
      "1  LSTM  0.474638  0.435855         0.64     0.380682   0.382850    0.64\n"
     ]
    }
   ],
   "source": [
    "#compare a specified experiment from each model using all sub-replicates\n",
    "importlib.reload(training_and_validation)\n",
    "import pandas as pd\n",
    "#set the experiment name for each model\n",
    "experiment_name_CNN = 'CNN_hyperparameter_search1/'\n",
    "experiment_name_LSTM = 'LSTM_hyperparameter_search1/'\n",
    "\n",
    "#initialize empty dataframe\n",
    "df = pd.DataFrame(columns=['model', 'accuracy', 'f1_score', 'sensitivity', 'specificity', 'precision', 'recall'])\n",
    "\n",
    "#get list of all the folders in the testing results directory\n",
    "architecture_folders = glob.glob('./testing_results/*/')\n",
    "\n",
    "#extract the data from each and make a dataframe comparing the n replicates\n",
    "for architecture in architecture_folders:\n",
    "\n",
    "    experiment_name = eval('experiment_name_'+architecture.split('/')[-2])[:-1]\n",
    "    print('experiment name: ', experiment_name)\n",
    "    #compute the mean and std of the accuracy, sensitivity, specificity, precision, recall, and f1 score for each replicate\n",
    "    accuracy_met, f1_score_met, sensitivity_met, specificity_met, precision_met, recall_met = get_metrics_from_testing_folder(path=architecture+experiment_name)\n",
    "    \n",
    "    #print the results\n",
    "    print('accuracy: ', np.round(accuracy_met[0],5), '+/-', np.round(accuracy_met[1],5))\n",
    "    print('f1 score: ', np.round(f1_score_met[0],5), '+/-', np.round(f1_score_met[1],5))\n",
    "    print('sensitivity: ', np.round(sensitivity_met[0],5), '+/-', np.round(sensitivity_met[1],5))\n",
    "    print('specificity: ', np.round(specificity_met[0],5), '+/-', np.round(specificity_met[1],5))\n",
    "    print('precision: ', np.round(precision_met[0],5), '+/-', np.round(precision_met[1],5))\n",
    "    print('recall: ', np.round(recall_met[0],5), '+/-', np.round(recall_met[1],5))\n",
    "\n",
    "\n",
    "    print('----------------------------------------------------------------')\n",
    "\n",
    "    #add to df\n",
    "    df.loc[len(df)] = [architecture.split('/')[-2], accuracy_met[0], f1_score_met[0], sensitivity_met[0], specificity_met[0], precision_met[0], recall_met[0]]\n",
    "print(df)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LSTM_hyperparameter_search1'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_filename"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7602579475998696e-95\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 236.22x157.48 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#subjectwise CV at the epoch level\n",
    "plot_confusion_matrix(TP=790, FP=259, TN=752, FN=321, filename='subjectwise_matrix_epochs')\n",
    "x2, p = run_chi_squared_test(TP=790, FP=259, TN=752, FN=321)\n",
    "print(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Single Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are this many items in the list of data  760\n",
      "there are this many items in the list of labels  760\n",
      "The length of the lists of channels means and stds is  60\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_workers = 2\n",
    "chunk_size = 2500\n",
    "#locate the raw data\n",
    "data_src =  './Data/UNM/all_data_reref_bandpass_1_to_45/'\n",
    "\n",
    "############ create list of subject numbers to leave out ###############################\n",
    "files = glob.glob(data_src + '*.csv')\n",
    "leave_one_out_list = []\n",
    "for file in files:  \n",
    "  leave_one_out_list.append(file.split('/')[-1])#.split('_')[1]) #remove hashtags to return to UNM dataset\n",
    "\n",
    "############# create dataset of all data ############################\n",
    "EEG_whole_Dataset = data_handling.EEGDataset(data_path=data_src, chunk_size=chunk_size)\n",
    "\n",
    "################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 60, 2500])\n",
      "there are this many batches in the training dataloader: 86\n",
      "torch.Size([8, 60, 2500])\n",
      "there are this many batches in the validation dataloader:  10\n"
     ]
    }
   ],
   "source": [
    "################ CREATE DATALOADER  ############################################\n",
    "#define the train test split\n",
    "train_size = int(0.90 * len(EEG_whole_Dataset))\n",
    "val_size = len(EEG_whole_Dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(EEG_whole_Dataset, [train_size, val_size],generator=torch.Generator().manual_seed(402))\n",
    "\n",
    "#create a respective dataloader out of the test/train split\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True, num_workers=num_workers)\n",
    "\n",
    "print(next(iter(train_dataloader))[0].size())\n",
    "print('there are this many batches in the training dataloader:',len(train_dataloader))\n",
    "print(next(iter(validation_dataloader))[0].size())\n",
    "print('there are this many batches in the validation dataloader: ',len(validation_dataloader))\n",
    "#the trainlaoder has 91 batches, the valloader has 16 batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(models)\n",
    "import models\n",
    "from models import PD_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 60, 2500])\n",
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# construct a nn.Module class for the model\n",
    "input_tensor = torch.rand([8,60,2500]) #of the form [batch_size, channels, epoch_length]\n",
    "print(input_tensor.size())\n",
    "network = PD_LSTM().to(device)\n",
    "output_tensor = network(input_tensor)\n",
    "print((output_tensor.shape))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batch:    86] average training loss: 0.601\n",
      "true positives:  0\n",
      "false positives:  0\n",
      "true negatives:  57\n",
      "false negatives 19\n",
      "The vote was:  Correct\n",
      "Finished Training Session\n",
      "The training loss at the end of this session is:  0.6182800531387329\n"
     ]
    }
   ],
   "source": [
    "train_model, training_loss_tracker, val_loss_tracker = train_with_validation(network, train_dataloader=training_dataloader, val_dataloader=validation_dataloader, epochs=1, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are this many items in the list of data  304\n",
      "there are this many items in the list of labels  304\n",
      "The length of the lists of channels means and stds is  60\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_workers = 2\n",
    "chunk_size = 2500\n",
    "#locate the raw data\n",
    "data_src =  '/Users/rakan/ResearchPD/DL_for_EEG/Data/UI/all_data_reref_bandpass_1_to_45/'\n",
    "\n",
    "############ create list of subject numbers to leave out ###############################\n",
    "files = glob.glob(data_src + '*.csv')\n",
    "leave_one_out_list = []\n",
    "for file in files:  \n",
    "  leave_one_out_list.append(file.split('/')[-1])#.split('_')[1]) #remove hashtags to return to UI dataset\n",
    "\n",
    "############# create dataset of all data ############################\n",
    "EEG_test_Dataset = data_handling.EEGDataset(data_path=data_src, chunk_size=chunk_size)\n",
    "\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 60, 2500])\n",
      "there are this many batches in the validation dataloader:  38\n"
     ]
    }
   ],
   "source": [
    "################ CREATE DATALOADER  ############################################\n",
    "\n",
    "#create a respective dataloader out of the test/train split\n",
    "test_loader = DataLoader(EEG_test_Dataset, batch_size=batch_size,shuffle=True, num_workers=num_workers)\n",
    "\n",
    "print(next(iter(test_loader))[0].size())\n",
    "print('there are this many batches in the validation dataloader: ',len(test_loader))\n",
    "#the trainlaoder has 91 batches, the valloader has 16 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives:  0\n",
      "false positives:  0\n",
      "true negatives:  160\n",
      "false negatives 144\n",
      "The vote was:  Correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 0,\n",
       " 160,\n",
       " 144,\n",
       " 'Correct',\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model=train_model, valloader=test_loader, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leave_one_out_list = []\n",
    "for file in files:  \n",
    "  leave_one_out_list.append(file.split('/')[-1])#.split('_')[1]) #remove hashtags to return to UNM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a fold while leaving out:  CTL_902.csv\n",
      "The vote was:  Correct\n",
      "True Positives:  0\n",
      "False Positives:  0\n",
      "True Negatives:  42\n",
      "False Negatives:  0\n",
      "Running a fold while leaving out:  CTL_903.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# build a function to perform subjectwise cross validation.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# loso_cross_validation\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m loso_cross_validation(filename_list\u001b[39m=\u001b[39;49mleave_one_out_list, EEG_whole_Dataset\u001b[39m=\u001b[39;49mEEG_whole_Dataset, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/ResearchPD/DL_for_EEG/training_and_validation.py:336\u001b[0m, in \u001b[0;36mloso_cross_validation\u001b[0;34m(filename_list, EEG_whole_Dataset, batch_size, num_workers, chunk_size, device)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(EEG_whole_Dataset)):\n\u001b[0;32m--> 336\u001b[0m   complete_list \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(EEG_whole_Dataset))\n\u001b[1;32m    338\u001b[0m   subset_ds \u001b[39m=\u001b[39m Subset(EEG_whole_Dataset, [index])\n\u001b[1;32m    339\u001b[0m   sample_sampler \u001b[39m=\u001b[39m RandomSampler(subset_ds)\n",
      "File \u001b[0;32m~/ResearchPD/DL_for_EEG/training_and_validation.py:216\u001b[0m, in \u001b[0;36mcross_train\u001b[0;34m(train_dataloader, val_dataloader, epochs, learning_rate, num_workers, threshold, chunk_size, device)\u001b[0m\n\u001b[1;32m    213\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    214\u001b[0m counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 216\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m    217\u001b[0m     \n\u001b[1;32m    218\u001b[0m     \u001b[39m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     inputs, labels, filename \u001b[39m=\u001b[39m data\n\u001b[1;32m    220\u001b[0m     inputs, labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpermute(inputs,(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device) \u001b[39m#send them to the GPU\u001b[39;00m\n",
      "File \u001b[0;32m~/ResearchPD/DL_for_EEG/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/ResearchPD/DL_for_EEG/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1317\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[39m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shutdown_workers()\n\u001b[1;32m   1318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[39m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \n\u001b[1;32m   1322\u001b[0m \u001b[39m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/ResearchPD/DL_for_EEG/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1442\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1438\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers:\n\u001b[1;32m   1439\u001b[0m     \u001b[39m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[39m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[39m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m     w\u001b[39m.\u001b[39;49mjoin(timeout\u001b[39m=\u001b[39;49m_utils\u001b[39m.\u001b[39;49mMP_STATUS_CHECK_INTERVAL)\n\u001b[1;32m   1443\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues:\n\u001b[1;32m   1444\u001b[0m     q\u001b[39m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m wait([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinel], timeout):\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# build a function to perform subjectwise cross validation.\n",
    "# loso_cross_validation\n",
    "loso_cross_validation(filename_list=leave_one_out_list, EEG_whole_Dataset=EEG_whole_Dataset, epochs=1, device=device)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these already exist, you just have to dig around in the notebooks to find them\n",
    "# Confusion matrix\n",
    "# ROC curve\n",
    "# waterfall plot\n",
    "# sequence plot\n",
    "# combo plot\n",
    "# training curves (loss and validation trackers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(data_src + '*.csv')\n",
    "subject_list = []\n",
    "filename_list = []\n",
    "for file in files:  \n",
    "  filename = file.split('/')[-1] #remove all preceeding directories\n",
    "  \n",
    "  filebasename = filename.split('.')[0] #drop the .csv\n",
    "  \n",
    "  subject_number = filebasename[-4:] #last four will be the subject number\n",
    "  \n",
    "  subject_list.append(subject_number)\n",
    "  filename_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from result_visualization import make_combo_plot\n",
    "#ax, fig = make_combo_plot(, filename_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wishlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 60, 2500])\n",
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, channels, time_points):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = time_points\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (channels, 60), padding=0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "\n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(16, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 2))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        # FC Layer\n",
    "        self.fc1 = None  # We'll initialize this later\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, x.shape[1], x.shape[2])  # Reshape the input\n",
    "\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = x.permute(0, 1, 3, 2)  # Revised permute operation\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling3(x)\n",
    "\n",
    "        # Flatten the output from the last layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # If this is the first forward pass, initialize self.fc1 based on the size of x\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 2)\n",
    "\n",
    "        # FC Layer\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_tensor = torch.rand([8, 60, 2500]).to(device)\n",
    "print(input_tensor.size())\n",
    "\n",
    "channels = input_tensor.size(1)\n",
    "time_points = input_tensor.size(2)\n",
    "network = EEGNet(channels, time_points).to(device)\n",
    "\n",
    "output_tensor = network(input_tensor)\n",
    "print(output_tensor.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ricksugden/Desktop/EEG_analysis/DL_for_EEG\n",
      "Already up to date.\n",
      "Result: CD'ed into our DL_for_EEG repo and did a git pull\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isdir(\"/Users/ricksugden/Desktop/EEG_analysis/DL_for_EEG/\"):\n",
    "    %cd \"/Users/ricksugden/Desktop/EEG_analysis/DL_for_EEG/\"\n",
    "    !git pull \n",
    "    print(\"Result: CD'ed into our DL_for_EEG repo and did a git pull\")\n",
    "\n",
    "elif os.path.isdir(\"/Users/ricksugden/Desktop/EEG_analysis/\"):\n",
    "    %cd \"/Users/ricksugden/Desktop/EEG_analysis/\"\n",
    "    github_token = \"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\"\n",
    "    !git clone https://github.com/RickSugden/DL_for_EEG.git \n",
    "    %cd /Users/ricksugden/Desktop/EEG_analysis/DL_for_EEG\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_handling' from '/Users/rakan/ResearchPD/DL_for_EEG/data_handling.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import glob\n",
    "import data_handling\n",
    "import importlib\n",
    "importlib.reload(data_handling)\n",
    "#from data_handling import EEGDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are this many items in the list of data  760\n",
      "there are this many items in the list of labels  760\n",
      "The length of the lists of channels means and stds is  60\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_workers = 2\n",
    "chunk_size = 2500\n",
    "#locate the raw data\n",
    "data_src =  '/Users/rakan/ResearchPD/DL_for_EEG/Data/UNM/all_data_reref_bandpass_1_to_45/'\n",
    "\n",
    "############ create list of subject numbers to leave out ###############################\n",
    "files = glob.glob(data_src + '*.csv')\n",
    "leave_one_out_list = []\n",
    "for file in files:  \n",
    "  leave_one_out_list.append(file.split('/')[-1])#.split('_')[1]) #remove hashtags to return to UNM dataset\n",
    "\n",
    "############# create dataset of all data ############################\n",
    "EEG_whole_Dataset = EEGDataset(data_path=data_src, chunk_size=chunk_size)\n",
    "\n",
    "################################################3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dataset class and use to make a dataloader\n",
    "\n",
    "#make dataset\n",
    "\n",
    "#make dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a nn.Module class for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "# ROC curve\n",
    "\n",
    "# waterfall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a function to perform subjectwise cross validation.\n",
    "# you can wrap this into a function and put it as a module in the repo\n",
    "##################### TESTING ###########################################\n",
    "'''\n",
    "Here, a for loop will iterate through every object in the whole dataset. Using the filename, it will determine the\n",
    "subject number for the sample and make a test set using only the one subject number. It will repeat this for each subject number.\n",
    "Epochs and Learning rate are adjustable below.\n",
    "'''\n",
    "model.eval()\n",
    "correct_votes, incorrect_votes, unsure_votes = 0,0,0\n",
    "true_positives, false_positives, true_negatives, false_negatives = 0,0,0,0\n",
    "subject_TP, subject_FP, subject_TN, subject_FN = 0, 0, 0, 0\n",
    "correct_epochs_list, incorrect_epochs_list = [], []\n",
    "list_of_sequences = []\n",
    "\n",
    "#leave_out will be \n",
    "for filename in filename_list:\n",
    "  sequence = []\n",
    "  to_test = []\n",
    "  print('Performing testing on subject number: ', filename.split('.')[0])\n",
    "\n",
    "  for index in range(len(EEG_whole_Dataset)):\n",
    "    \n",
    "    complete_list = range(len(EEG_whole_Dataset))\n",
    "    subset_ds = Subset(EEG_whole_Dataset, [index])\n",
    "    sample_sampler = RandomSampler(subset_ds)\n",
    "    subset_dataloader = DataLoader(subset_ds, sampler=sample_sampler, batch_size=1)\n",
    "    data = next(iter(subset_dataloader))\n",
    "    eeg_data, label, file = data\n",
    "    \n",
    "    if file[0] == filename:\n",
    "      \n",
    "      to_test.append(index)\n",
    "\n",
    "  #to_be_kept = [x for x in complete_list if x not in to_be_removed]\n",
    "  #train_dataset = Subset(EEG_whole_Dataset, to_be_kept)\n",
    "  \n",
    "  val_dataset = Subset(EEG_whole_Dataset, to_test)\n",
    "\n",
    "  \n",
    "  #train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "  \n",
    "  del val_dataset # ,train_dataset\n",
    "\n",
    "\n",
    "  TP, FP, TN, FN, vote, sequence = validate(model=model, valloader=val_dataloader,threshold=0.5,batch_size=1, supress_output=False)\n",
    "  correct_epochs_list.append(TP+TN) \n",
    "  incorrect_epochs_list.append(FP+FN) \n",
    "  print(len(val_dataloader))\n",
    "  true_positives += TP\n",
    "  false_positives += FP\n",
    "  true_negatives += TN\n",
    "  false_negatives += FN\n",
    "  if vote == 'Correct':\n",
    "    correct_votes += 1\n",
    "    if TP >0:\n",
    "      subject_TP += 1\n",
    "    elif TN >0:\n",
    "      subject_TN += 1\n",
    "    \n",
    "    sequence.append(1)\n",
    "    \n",
    "  elif vote == 'Incorrect':\n",
    "    incorrect_votes += 1\n",
    "    if FP >0:\n",
    "      subject_FP += 1\n",
    "    elif FN >0:\n",
    "      subject_FN += 1\n",
    "  else:\n",
    "    unsure_votes +=1\n",
    "    sequence.append(0)\n",
    "\n",
    "  list_of_sequences.append(sequence)\n",
    "  print('-----------------------------------------------------------------------')\n",
    "  \n",
    "print('total True Positives: ', true_positives)\n",
    "print('total False Positives: ', false_positives)\n",
    "print('total True Negatives: ', true_negatives)\n",
    "print('total False Negatives: ', false_negatives)\n",
    "print('total correct subject classifications: ', correct_votes)\n",
    "print('total incorrect subject classifications: ', incorrect_votes)\n",
    "print('total unsure subject classifications: ',unsure_votes)\n",
    "print('total subject level True positives: ', subject_TP)\n",
    "print('total subject level False Positives: ', subject_FP)\n",
    "print('total subject level True Negatives: ', subject_TN)\n",
    "print('total subject level False Negatives: ', subject_FN)\n",
    "subject_accuracy = (subject_TP + subject_TN) / (subject_TP + subject_FP +subject_TN+subject_FN)\n",
    "print('total subject accuracy: ', subject_accuracy)\n",
    "subject_F1 = 2*subject_TP / (2*subject_TP + subject_FP + subject_FN)\n",
    "print('total subject F1: ', subject_F1)\n",
    "subject_sensitvity = subject_TP / (subject_TP + subject_FN)\n",
    "print('total subject sensitivity: ',subject_sensitvity)\n",
    "subject_specificity = subject_TN / (subject_FP + subject_TN)\n",
    "print('total subject specificity: ', subject_specificity)\n",
    "\n",
    "print('----------------------------------------------------------------')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these already exist, you just have to dig around in the notebooks to find them\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "# ROC curve\n",
    "\n",
    "# waterfall plot\n",
    "\n",
    "# sequence plot\n",
    "\n",
    "# training curves (loss and validation trackers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wishlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automated hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
